{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Feedforward import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28, 1)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.reshape(batch_X, [-1,28,28,1]).shape)\n",
    "print(batch_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_size= [28, 28]\n",
    "n_outputs=  batch_y.shape[1]\n",
    "n_input_maps= 1\n",
    "\n",
    "n_filters = [32, 64]   \n",
    "filter_size = [[5,5], [5,5]] \n",
    "pool_size =   [[2,2], [2,2]] \n",
    "n_hidden = [1024]\n",
    "\n",
    "batch_size= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the maps after convolution stage: [4, 4]\n",
      "Shape of input matrix entering to Fully connected layers: [100, 4, 4, 64]\n"
     ]
    }
   ],
   "source": [
    "# for dropout\n",
    "drop_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Define ConvNet\n",
    "conv_net= ConvNet( input_size, n_input_maps, n_outputs, n_filters, filter_size, pool_size, n_hidden, name='ConvNet')\n",
    "\n",
    "# Define train setup\n",
    "train= conv_net.setup(batch_size, drop_prob, loss_type= 'cross_entropy')\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.AdamOptimizer(1e-3).minimize(train.loss) #0.001\n",
    "  \n",
    "# Predictions for the training, validation, and test data.\n",
    "train_prediction = tf.nn.softmax(train.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    if n_outputs == 1:\n",
    "        return (100.0 * np.sum(np.greater(predictions, 0.5) == np.greater(labels, 0.5))/ predictions.shape[0])\n",
    "    else:\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/ predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 , train: 0.12  | test: 13.0  | loss: 0.0563579177856\n",
      "100 , train: 85.74  | test: 94.1  | loss: 0.619109061882\n",
      "200 , train: 96.59  | test: 95.6  | loss: 0.111527744029\n",
      "300 , train: 97.21  | test: 97.3  | loss: 0.0884608231485\n",
      "400 , train: 97.54  | test: 96.2  | loss: 0.0846089943312\n",
      "500 , train: 97.45  | test: 98.9  | loss: 0.0825030921027\n",
      "600 , train: 98.62  | test: 99.2  | loss: 0.0446857105772\n",
      "700 , train: 98.84  | test: 99.8  | loss: 0.0370924306102\n",
      "800 , train: 98.69  | test: 99.7  | loss: 0.0404313161154\n",
      "900 , train: 98.82  | test: 98.9  | loss: 0.0328410544875\n",
      "1000 , train: 98.89  | test: 98.0  | loss: 0.0376981027861\n",
      "1100 , train: 98.89  | test: 98.7  | loss: 0.0353011970699\n",
      "1200 , train: 99.25  | test: 99.4  | loss: 0.0225167475408\n",
      "1300 , train: 99.17  | test: 99.2  | loss: 0.0249677124067\n",
      "1400 , train: 99.29  | test: 98.5  | loss: 0.0219973465032\n",
      "1500 , train: 99.2  | test: 99.1  | loss: 0.0233294674236\n",
      "1600 , train: 99.26  | test: 98.8  | loss: 0.0257655075938\n",
      "1700 , train: 99.45  | test: 98.5  | loss: 0.0203667616911\n",
      "1800 , train: 99.57  | test: 98.6  | loss: 0.0151066194545\n",
      "1900 , train: 99.4  | test: 99.0  | loss: 0.0169331504175\n",
      "2000 , train: 99.39  | test: 98.4  | loss: 0.0154177341977\n",
      "2100 , train: 99.29  | test: 98.3  | loss: 0.0247297829087\n",
      "2200 , train: 99.47  | test: 99.3  | loss: 0.0178367159737\n",
      "2300 , train: 99.59  | test: 99.1  | loss: 0.0111638780325\n",
      "2400 , train: 99.75  | test: 99.5  | loss: 0.00960091434245\n",
      "2500 , train: 99.59  | test: 98.6  | loss: 0.0139330075115\n",
      "2600 , train: 99.54  | test: 98.9  | loss: 0.0150067645132\n",
      "2700 , train: 99.45  | test: 99.0  | loss: 0.016594944254\n",
      "2800 , train: 99.58  | test: 99.2  | loss: 0.0123821983798\n",
      "2900 , train: 99.86  | test: 99.5  | loss: 0.00448340838178\n",
      "3000 , train: 99.5  | test: 99.1  | loss: 0.0127298788252\n",
      "3100 , train: 99.64  | test: 98.4  | loss: 0.0126635758323\n",
      "3200 , train: 99.6  | test: 99.3  | loss: 0.0116622750064\n",
      "3300 , train: 99.63  | test: 98.5  | loss: 0.0109262838985\n",
      "3400 , train: 99.68  | test: 99.1  | loss: 0.00877909513729\n",
      "3500 , train: 99.53  | test: 98.8  | loss: 0.0146405788518\n",
      "3600 , train: 99.77  | test: 99.2  | loss: 0.00706877623059\n",
      "3700 , train: 99.49  | test: 98.9  | loss: 0.0151519713526\n",
      "3800 , train: 99.71  | test: 99.3  | loss: 0.00882628465864\n",
      "3900 , train: 99.64  | test: 99.3  | loss: 0.0100612769949\n",
      "4000 , train: 99.69  | test: 99.1  | loss: 0.00925112769677\n",
      "4100 , train: 99.86  | test: 99.4  | loss: 0.00493527559443\n",
      "4200 , train: 99.8  | test: 99.2  | loss: 0.00651521185913\n",
      "4300 , train: 99.72  | test: 99.1  | loss: 0.00841992457528\n",
      "4400 , train: 99.77  | test: 99.1  | loss: 0.0074403661926\n",
      "4500 , train: 99.79  | test: 99.6  | loss: 0.00623767827957\n",
      "4600 , train: 99.73  | test: 98.5  | loss: 0.00744682778435\n",
      "4700 , train: 99.66  | test: 98.7  | loss: 0.0108436151099\n",
      "4800 , train: 99.66  | test: 99.2  | loss: 0.00898969366192\n",
      "4900 , train: 99.56  | test: 98.6  | loss: 0.0149581384365\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "summary_freq= 100\n",
    "n_test_logg= 10 # number of evaluations on test dataset (for logging information)\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "print('Initialized')\n",
    "\n",
    "mean_loss= 0\n",
    "train_accuracy= 0\n",
    "for step in range(num_steps):\n",
    "    batch_X, batch_y= mnist.train.next_batch(batch_size)\n",
    "    batch_X= np.reshape(batch_X, [-1,28,28,1])\n",
    "    \n",
    "    \n",
    "    feed_dict = {train.inputs : batch_X, train.labels : batch_y, drop_prob:1.0}\n",
    "    \n",
    "    _, l, train_pred = sess.run([optimizer, train.loss, train_prediction],feed_dict=feed_dict)\n",
    "    \n",
    "    train_accuracy += accuracy(train_pred, batch_y)\n",
    "    mean_loss += l    \n",
    "    \n",
    "    if step%summary_freq == 0:\n",
    "        # train\n",
    "        train_accuracy= train_accuracy/summary_freq\n",
    "        \n",
    "        # test\n",
    "        test_accuracy= 0\n",
    "        for i in range(n_test_logg):\n",
    "            batch_X_test, batch_y_test= mnist.test.next_batch(batch_size) \n",
    "            batch_X_test= np.reshape(batch_X_test, [-1,28,28,1])\n",
    "\n",
    "            pred = train_prediction.eval(feed_dict={train.inputs: batch_X_test, drop_prob: 1.0})\n",
    "            \n",
    "            test_accuracy += accuracy(pred, batch_y_test)\n",
    "        test_accuracy= test_accuracy/n_test_logg\n",
    "            \n",
    "        print(step, ', train:',train_accuracy,' | test:', test_accuracy, ' | loss:', mean_loss/summary_freq)\n",
    "        mean_loss= 0\n",
    "        train_accuracy= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
