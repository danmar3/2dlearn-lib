{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GMM\n",
    "Created by: Daniel L. Marino (marinodl@vcu.edu)\n",
    "\n",
    "Description: gaussian mixture model implementation on tensorflow, the objective is a conditional probabilistic distribution. \n",
    "\n",
    "This file serves as a visual test for the gmm extension modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from twodlearn.tf_lib.datasets.generic import Datasets\n",
    "from twodlearn.tf_lib.GMM import *\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def gmm_sampling(mu, sigma, w, n_samples=1):\n",
    "    # generates random vectors, sampled from a gausian mixture model\n",
    "    #  \n",
    "    #     - mu: 3d tensor containing the means for the gaussians.\n",
    "    #           the \"depth\" dimention (3rd) is used to index the\n",
    "    #           gaussians.    [ kernel_id, dim]\n",
    "    #     - sigma: 3d tensor containing the covariance matrix of the\n",
    "    #              gaussians. [ kernel_id, dim] for diagonal matrices\n",
    "    #     - w: vector in form of a 3d tensor containing the weights\n",
    "    #          for each one of the gaussians, they have to sum one. \n",
    "    #          [kernel_id]\n",
    "    n_kernels = mu.shape[0]\n",
    "    n_dim = mu.shape[1]\n",
    "    \n",
    "    # random sample the kernel from which the output samples are going\n",
    "    # to be drawn\n",
    "    kernel_id= np.argmax(np.random.multinomial(1, w, size=[n_samples]), axis=1 )\n",
    "    \n",
    "    out = np.zeros([n_samples, n_dim])\n",
    "    for i in range(n_samples):\n",
    "        out[i,:]= np.random.multivariate_normal(mu[kernel_id[i],:], np.diag(sigma[kernel_id[i],:])) # if diagonal\n",
    "    \n",
    "    return out;\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (1000000,)\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000000\n",
    "\n",
    "sigma_r = 1.0*np.exp(-0.005*np.linspace(0, 1000, n_samples))\n",
    "w_r = [1]  # has to sum to one\n",
    "\n",
    "aux_x= np.linspace(0, 6, n_samples)\n",
    "aux_y= np.linspace(0, 6, n_samples)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    aux_y[i] = np.random.normal(np.sin( aux_x[i] ), sigma_r[i])\n",
    "\n",
    "# build the dataset\n",
    "data = Datasets(aux_x, aux_y)\n",
    "data.normalize()\n",
    "print('Data shape: ', data.train.x.shape)\n",
    "\n",
    "# plot\n",
    "#plt.plot(data.train.x, data.train.y, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.20000119997e-05\n"
     ]
    }
   ],
   "source": [
    "print(np.sin(aux_x[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#config = tf.ConfigProto( device_count = {'GPU': 0} )\n",
    "#sess = tf.InteractiveSession(config = config)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    Args:\n",
    "        tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "        is over individual gradients. The inner list is over the gradient\n",
    "        calculation for each tower.\n",
    "    Returns:\n",
    "        List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "        across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    # 1. loop through each variable\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        # 2. loop through each tower\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            \n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "            \n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(0, grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "        \n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "        \n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_inputs = 1\n",
    "n_outputs = 1\n",
    "n_hidden= [4]\n",
    "n_kernels = 1\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    gmm_model = GmmMlpModel( n_inputs, n_outputs, n_hidden, n_kernels, \n",
    "                             afunction= tf.sigmoid, diagonal= True, method='tdl')\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = tf.train.AdamOptimizer(0.002)\n",
    "    \n",
    "# Create models for each gpu\n",
    "tower_grads = []\n",
    "train_models = []\n",
    "with tf.device('/gpu:0'):\n",
    "    train_model = gmm_model.setup(n_samples/2)\n",
    "    train_models.append( train_model )\n",
    "    \n",
    "    grads = optimizer.compute_gradients(train_model.loss)\n",
    "    \n",
    "    tower_grads.append(grads)\n",
    "    \n",
    "with tf.device('/gpu:1'):\n",
    "    train_model = gmm_model.setup(n_samples/2)\n",
    "    train_models.append( train_model )\n",
    "    \n",
    "    grads = optimizer.compute_gradients(train_model.loss)\n",
    "    \n",
    "    tower_grads.append(grads)\n",
    "\n",
    "# average gradients\n",
    "grads = average_gradients(tower_grads)\n",
    "\n",
    "# apply gradients\n",
    "apply_gradient_op = optimizer.apply_gradients(grads);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu shape: (500000, 1, 1)\n",
      "sigma shape: (500000, 1, 1)\n",
      "w shape: (500000, 1)\n",
      "out shape: <unknown>\n",
      "loss shape: <unknown>\n"
     ]
    }
   ],
   "source": [
    "print('mu shape:', train_model.mu.get_shape())\n",
    "print('sigma shape:', train_model.sigma.get_shape())\n",
    "print('w shape:', train_model.w.get_shape())\n",
    "\n",
    "print('out shape:', train_model.out.get_shape())\n",
    "\n",
    "print('loss shape:', train_model.loss.get_shape())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinodl/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/marinodl/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:16: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/marinodl/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:17: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/marinodl/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  | loss: 0.0129496192932\n",
      "100  | loss: 1.21944937229\n",
      "200  | loss: 1.0807075417\n",
      "300  | loss: 0.862729559541\n",
      "400  | loss: 0.73187566936\n",
      "500  | loss: 0.667426918149\n",
      "600  | loss: 0.610249723196\n",
      "700  | loss: 0.545957196355\n",
      "800  | loss: 0.484142828584\n",
      "900  | loss: 0.432452298999\n",
      "1000  | loss: 0.383267418742\n",
      "1100  | loss: 0.320830774903\n",
      "1200  | loss: 0.203391505629\n",
      "1300  | loss: -0.017459448576\n",
      "1400  | loss: -0.285283521712\n",
      "1500  | loss: -0.517055316865\n",
      "1600  | loss: -0.697840419412\n",
      "1700  | loss: -0.826660934687\n",
      "1800  | loss: -0.905860103965\n",
      "1900  | loss: -0.947606394291\n",
      "2000  | loss: -0.971997644305\n",
      "2100  | loss: -0.989161190987\n",
      "2200  | loss: -1.00185326159\n",
      "2300  | loss: -1.01124534726\n",
      "2400  | loss: -1.01819988608\n",
      "2500  | loss: -1.02367614269\n",
      "2600  | loss: -1.02792557359\n",
      "2700  | loss: -1.03160936713\n",
      "2800  | loss: -1.03461680651\n",
      "2900  | loss: -1.03742195606\n",
      "3000  | loss: -1.03966118097\n",
      "3100  | loss: -1.04194389701\n",
      "3200  | loss: -1.04380267382\n",
      "3300  | loss: -1.04562702537\n",
      "3400  | loss: -1.04712856174\n",
      "3500  | loss: -1.04865194201\n",
      "3600  | loss: -1.04985672116\n",
      "3700  | loss: -1.05112405896\n",
      "3800  | loss: -1.05216197371\n",
      "3900  | loss: -1.05314240098\n",
      "4000  | loss: -1.05416719556\n",
      "4100  | loss: -1.05491541266\n",
      "4200  | loss: -1.05584136724\n",
      "4300  | loss: -1.05649418354\n",
      "4400  | loss: -1.0573180294\n",
      "4500  | loss: -1.05791503191\n",
      "4600  | loss: -1.05855743527\n",
      "4700  | loss: -1.05927655816\n",
      "4800  | loss: -1.05973292947\n",
      "4900  | loss: -1.06043053389\n",
      "5000  | loss: -1.06085107923\n",
      "5100  | loss: -1.06148785353\n",
      "5200  | loss: -1.0618549633\n",
      "5300  | loss: -1.06245595932\n",
      "5400  | loss: -1.06278807163\n",
      "5500  | loss: -1.06334492564\n",
      "5600  | loss: -1.06353737116\n",
      "5700  | loss: -1.06414990306\n",
      "5800  | loss: -1.06453315973\n",
      "5900  | loss: -1.06470415831\n",
      "6000  | loss: -1.06525111556\n",
      "6100  | loss: -1.06546047449\n",
      "6200  | loss: -1.06591362119\n",
      "6300  | loss: -1.06613170624\n",
      "6400  | loss: -1.06654580593\n",
      "6500  | loss: -1.0667670083\n",
      "6600  | loss: -1.06716108084\n",
      "6700  | loss: -1.06730718613\n",
      "6800  | loss: -1.06774138331\n",
      "6900  | loss: -1.06787238955\n",
      "7000  | loss: -1.06830523133\n",
      "7100  | loss: -1.06844615936\n",
      "7200  | loss: -1.06886098266\n",
      "7300  | loss: -1.0690138948\n",
      "7400  | loss: -1.06941372752\n",
      "7500  | loss: -1.06953387737\n",
      "7600  | loss: -1.06996560693\n",
      "7700  | loss: -1.0701268363\n",
      "7800  | loss: -1.07043558836\n",
      "7900  | loss: -1.07076438785\n",
      "8000  | loss: -1.07096157074\n",
      "8100  | loss: -1.07135596752\n",
      "8200  | loss: -1.07152537704\n",
      "8300  | loss: -1.07184026837\n",
      "8400  | loss: -1.0721674335\n",
      "8500  | loss: -1.07232744694\n",
      "8600  | loss: -1.07275139928\n",
      "8700  | loss: -1.07291008234\n",
      "8800  | loss: -1.07329903364\n",
      "8900  | loss: -1.07344324231\n",
      "9000  | loss: -1.07383488417\n",
      "9100  | loss: -1.07392647624\n",
      "9200  | loss: -1.07435173869\n",
      "9300  | loss: -1.07447307587\n",
      "9400  | loss: -1.07484731793\n",
      "9500  | loss: -1.07496484518\n",
      "9600  | loss: -1.07520849705\n",
      "9700  | loss: -1.07552368283\n",
      "9800  | loss: -1.07564841032\n",
      "9900  | loss: -1.07596707225\n",
      "function takes:  752.9586820602417\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000 #1000\n",
    "n_logging = 100\n",
    "n_test_logg = 10\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "print('Initialized')\n",
    "\n",
    "mean_loss= 0\n",
    "train_accuracy= 0\n",
    "\n",
    "t0 = time()\n",
    "for step in range(num_steps):   \n",
    "    \n",
    "    _, l1, l2 = sess.run([apply_gradient_op, train_models[0].loss, train_models[1].loss],\n",
    "                         feed_dict={train_models[0].inputs : np.expand_dims(data.train.x[:n_samples/2],1),\n",
    "                                    train_models[0].labels : np.expand_dims(data.train.y[:n_samples/2],1),\n",
    "                                    train_models[1].inputs : np.expand_dims(data.train.x[n_samples/2:],1),\n",
    "                                    train_models[1].labels : np.expand_dims(data.train.y[n_samples/2:],1),\n",
    "                                   })\n",
    "    mean_loss += 0.5*(l1 + l2)\n",
    "    \n",
    "    \n",
    "    if step%n_logging == 0:                \n",
    "        print(step, ' | loss:', mean_loss/n_logging)\n",
    "        mean_loss= 0\n",
    "        \n",
    "t1 = time()\n",
    "print('function takes: ', (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model for testing\n",
    "n_test= 100;\n",
    "test_model = gmm_model.setup(n_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "x_plot = np.linspace(-2, 2, n_test)\n",
    "\n",
    "[mu_out, sigma_out, w_out,] = sess.run([ test_model.mu, test_model.sigma, test_model.w ],\n",
    "                                         feed_dict= {test_model.inputs : np.expand_dims(x_plot, 1)})\n",
    "#print('mu:', mu_out, 'sigma:', sigma_out, 'w:', w_out)\n",
    "\n",
    "print(np.squeeze(mu_out).shape)\n",
    "print(x_plot.shape)\n",
    "\n",
    "# plot\n",
    "#plt.plot(x_plot, np.squeeze(mu_out), 'o')\n",
    "\n",
    "#plt.plot(x_plot, np.squeeze(mu_out) + np.squeeze(np.sqrt(sigma_out)), 'ro')\n",
    "#plt.plot(x_plot, np.squeeze(mu_out) - np.squeeze(np.sqrt(sigma_out)), 'ro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
